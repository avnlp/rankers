python src/rankers/pipelines/listwise_ranking.py \
    --dataset_name "msmarco-document-v2/trec-dl-2020" \
    --model_path "castorini/rank_zephyr_7b_v1_full" \
    --ranker_type "zephyr" \
    --context_size 4096 \
    --num_few_shot_examples 0 \
    --top_k 25 \
    --device "cuda" \
    --num_gpus 1 \
    --sliding_window_size 5 \
    --sliding_window_step 2 \
    --system_message "You are RankLLM, an intelligent assistant that can rank passages based on their relevancy to the query." \
    --embedder_model "hkunlp/instructor-xl" \
    --query_embedding_instruction "Represent the question for retrieving supporting documents:" \
    --embedder_kwargs "{'device': 'cuda', 'normalize_embeddings': True}" \
    --milvus_connection_uri "MILVUS_URI" \
    --milvus_connection_token "MILVUS_TOKEN" \
    --milvus_document_store_kwargs "{'collection_name': 'trec20-test'}" \
    --documents_to_retrieve 50 \
    --cutoff_values 1 3 5 10 \
    --ignore_identical_ids \
    --decimal_precision 4 \
    --metrics_to_compute ndcg map recall precision
